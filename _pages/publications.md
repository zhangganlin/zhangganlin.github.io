
<!-- <hr>

  You can also find my articles on my <a href="https://scholar.google.com/citations?user=1B_T56IAAAAJ" target="_blank">Google Scholar profile</a>.

<hr> -->


<heading><strong>ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="vista">
      <div class="one">
        <div class="two shape" style="width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="https://ganlinzhang.xyz/vista-slam/media/vista_slam.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="/images/publications/vista.png" style="width: 100%;" class="img"/>
      </div>       
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://ganlinzhang.xyz/vista-slam" target="_blank">
            ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association
          </a>
        </strong>
      </papertitle>
      <br>
      <strong>Ganlin Zhang</strong>, 
      <a href="https://shenhanqian.github.io/" target="_blank">Shenhan Qian</a>, 
      <a href="https://xiwang1212.github.io/homepage/" target="_blank">Xi Wang</a>, 
      <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
      <br>
      <em>Preprint on ArXiv, 2025</em>
      <br>
      <a href="https://github.com/zhangganlin/vista-slam" target="_blank">Github Repo (Coming Soon)</a> | 
      <a href="https://arxiv.org/abs/2509.01584" target="_blank">ArXiv</a> | 
      <a href="https://ganlinzhang.xyz/vista-slam/" target="_blank">Project website</a>
      <br>
      ViSTA-SLAM is a real-time monocular dense SLAM pipeline that combines a Symmetric Two-view Association (STA) frontend with Sim(3) pose graph optimization and loop closure, enabling accurate camera trajectories and high-quality 3D scene reconstruction from RGB inputs.
    </td>
  </tr>
</table>
<hr>


<heading><strong>Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="batrack">
      <div class="one">
        <div class="two shape" style="width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="https://wrchen530.github.io/projects/batrack/static/videos/davis_3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="https://wrchen530.github.io/images/batrack.png" style="width: 100%;" class="img"/>
      </div>       
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://wrchen530.github.io/projects/batrack" target="_blank">
            Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction
          </a>
        </strong>
      </papertitle>
      <br>
      <a href="https://wrchen530.github.io/" target="_blank">Weirong Chen</a>, 
      <strong>Ganlin Zhang</strong>, 
      <a href="https://fwmb.github.io/" target="_blank">Felix Wimbauer</a>, 
      <a href="https://rui2016.github.io/" target="_blank">Rui Wang</a>, 
      <a href="https://arnike.github.io/" target="_blank">Nikita Araslanov</a>, 
      <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank">Andrea Vedaldi</a>, 
      <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
      <br>
      <em><strong>ICCV</strong> 2025 <span style="color:rgb(255, 94, 0);font-weight: bold;">(oral)</span></em>
      <br>
      <a href="https://wrchen530.github.io/projects/batrack/" target="_blank">Github Repo (Coming Soon)</a> | 
      <a href="https://arxiv.org/abs/2504.14516" target="_blank">ArXiv</a> | 
      <a href="https://wrchen530.github.io/projects/batrack/" target="_blank">Project website</a>
      <br>
      A method for consistent dynamic scene reconstruction via motion decoupling, bundle adjustment, and global refinement.
    </td>
  </tr>
</table>
<hr>


<heading><strong>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians</strong> </heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/splatslam.jpg" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/papers/Sandstrom_Splat-SLAM_Globally_Optimized_RGB-only_SLAM_with_3D_Gaussians_CVPRW_2025_paper.pdf" target="_blank">Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://eriksandstroem.github.io/" target="_blank">Erik Sandström*</a>, 
    <strong>Ganlin Zhang*</strong>, 
    <a href="https://scholar.google.com/citations?user=ml3laqEAAAAJ" target="_blank"> Keisuke Tateno</a>, 
    <a href="https://moechsle.github.io/" target="_blank"> Michael Oechsle</a>, 
    <a href="https://youmi-zym.github.io/" target="_blank"> Youmin Zhang</a>, 
    <a href="https://manthan99.github.io/" target="_blank"> Manthan Patel</a>, 
    <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjg3LC0xOTcxNDY1MTc4.html" target="_blank"> Luc Van Gool</a>, 
    <a href="https://oswaldm.github.io/" target="_blank"> Martin R. Oswald</a>, 
    <a href="https://federicotombari.github.io/" target="_blank"> Federico Tombari</a>
    <br>
    <em><strong>CVPRW</strong> 2025</em>
    <br>
    <a href="https://github.com/google-research/Splat-SLAM" target="_blank">Github Repo</a> | 
    <a href="https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/papers/Sandstrom_Splat-SLAM_Globally_Optimized_RGB-only_SLAM_with_3D_Gaussians_CVPRW_2025_paper.pdf" target="_blank">Paper</a> | 
    <a href="https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/supplemental/Sandstrom_Splat-SLAM_Globally_Optimized_CVPRW_2025_supplemental.pdf" target="_blank">Supp</a>
    <br>
    We use a keyframe based frame to frame tracker based on dense optical flow connected to a pose graph for global consistency. For dense mapping, we resort to a 3DGS representation, suitable for extracting both dense geometry and rendering from.
  </td>
</table>
<hr>

  
<heading><strong>GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="glorie">
      <div class="one">
        <div class="two shape" style="display: flex; justify-content: center; width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="/images/publications/glorie.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="/images/publications/glorie.jpg" width="100%" class="img"/>
      </div>        
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://zhangganlin.github.io/GlORIE-SLAM/index.html" target="_blank">
            GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM
          </a>
        </strong>
      </papertitle>
      <br>
      <strong>Ganlin Zhang*</strong>, 
      <a href="https://eriksandstroem.github.io/" target="_blank">Erik Sandström*</a>, 
      <a href="https://youmi-zym.github.io/" target="_blank">Youmin Zhang</a>, 
      <a href="https://manthan99.github.io/" target="_blank">Manthan Patel</a>, 
      <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjg3LC0xOTcxNDY1MTc4.html" target="_blank">Luc Van Gool</a>, 
      <a href="https://oswaldm.github.io/" target="_blank">Martin R. Oswald</a>
      <br>
      <em>Preprint on ArXiv, 2024</em>
      <br>
      <a href="https://github.com/zhangganlin/GlORIE-SLAM" target="_blank">Github Repo</a> | 
      <a href="https://arxiv.org/abs/2403.19549" target="_blank">ArXiv</a> | 
      <a href="https://zhangganlin.github.io/GlORIE-SLAM/index.html" target="_blank">Project website</a>
      <br>
      1. A monocular SLAM pipeline with deformable neural point cloud scene representation. <br>
      2. Novel DSPO layer for BA, which can jointly optimize depth map, depth scale, and camera pose. <br>
    </td>
  </tr>
</table>
<hr>


<heading><strong>Revisiting Rotation Averaging: Uncertainties and Robust Losses</strong> </heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/rotationAverage.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Revisiting_Rotation_Averaging_Uncertainties_and_Robust_Losses_CVPR_2023_paper.pdf" target="_blank">Revisiting Rotation Averaging: Uncertainties and Robust Losses</a>
    </strong>
    </papertitle>
    <br>
    <strong>Ganlin Zhang</strong>,
    <a href="https://vlarsson.github.io/" target="_blank">Viktor Larsson</a>,
    <a href="https://cvg.ethz.ch/team/Dr-Daniel-Bela-Barath" target="_blank">Daniel Barath</a>
    <br>
    <em><strong>CVPR</strong> 2023</em>
    <br>
    <a href="https://github.com/zhangganlin/GlobalSfMpy" target="_blank">Github Repo</a> | 
    <a href="https://arxiv.org/abs/2303.05195" target="_blank">ArXiv</a>
    <br>
    1. Better model the underlying noise distributions by directly propagating the uncertainty from the point correspondences into the rotation averaging. <br>
    2. Integrate a variant of the MAGSAC++ loss into the rotation averaging, instead of using the classical robust losses.
  </td>
</table>
<hr>




