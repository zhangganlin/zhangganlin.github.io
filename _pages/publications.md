
<!-- <hr>

  You can also find my articles on my <a href="https://scholar.google.com/citations?user=1B_T56IAAAAJ" target="_blank">Google Scholar profile</a>.

<hr> -->

<heading><strong>NOVA3R: Non-pixel-aligned Visual Transformer for Amodal 3D Reconstruction</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%">
      <div class="one">
      <img src="https://wrchen530.github.io/images/nova3r.png" width="100%"> </div>
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://openreview.net/forum?id=c0QRZMKwSb" target="_blank">
            NOVA3R: Non-pixel-aligned Visual Transformer for Amodal 3D Reconstruction
          </a>
        </strong>
      </papertitle>
      <br>
      <a href="https://wrchen530.github.io/" target="_blank">Weirong Chen</a>, 
      <a href="https://physicalvision.github.io/people/~chuanxia" target="_blank">Chuanxia Zheng</a>, 
      <strong>Ganlin Zhang</strong>, 
      <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank">Andrea Vedaldi</a>, 
      <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
      <br>
      <em><strong>ICLR</strong> 2026</em>
      <br>
      <a href="https://openreview.net/forum?id=c0QRZMKwSb" target="_blank">Preprint</a>  
      <br>
      NOVA3R is a feed-forward method for non-pixel-aligned 3D reconstruction from unposed images that learns a global, view-agnostic scene representation via scene tokens and a diffusion-based 3D decoder, enabling complete and physically plausible geometry and outperforming state of the art in accuracy and completeness.
    </td>
  </tr>
</table>
<hr>


<heading><strong>ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="vista">
      <div class="one">
        <div class="two shape" style="width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="https://ganlinzhang.xyz/vista-slam/media/vista_slam.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="/images/publications/vista.png" style="width: 100%;" class="img"/>
      </div>       
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://ganlinzhang.xyz/vista-slam" target="_blank">
            ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association
          </a>
        </strong>
      </papertitle>
      <br>
      <strong>Ganlin Zhang</strong>, 
      <a href="https://shenhanqian.github.io/" target="_blank">Shenhan Qian</a>, 
      <a href="https://xiwang1212.github.io/homepage/" target="_blank">Xi Wang</a>, 
      <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
      <br>
      <em><strong>3DV</strong> 2026</em>
      <br>
      <a href="https://github.com/zhangganlin/vista-slam" target="_blank">Github Repo</a> | 
      <a href="https://arxiv.org/abs/2509.01584" target="_blank">ArXiv</a> | 
      <a href="https://ganlinzhang.xyz/vista-slam/" target="_blank">Project website</a>
      <br>
      ViSTA-SLAM is a real-time monocular dense SLAM pipeline that combines a Symmetric Two-view Association (STA) frontend with Sim(3) pose graph optimization and loop closure, enabling accurate camera trajectories and high-quality 3D scene reconstruction from RGB inputs.
    </td>
  </tr>
</table>
<hr>

<heading><strong>SNI-SLAM++: Tightly-coupled Semantic Neural Implicit SLAM</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="sniplus">
      <div class="one">
        <div class="two shape" style="width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="/images/publications/snislamplus.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="/images/publications/snislamplus.png" style="width: 100%;" class="img"/>
      </div>       
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://irmvlab.github.io/sni-slam-plus.github.io/" target="_blank">
            SNI-SLAM++: Tightly-coupled Semantic Neural Implicit SLAM
          </a>
        </strong>
      </papertitle>
      <br>
      <a href="https://scholar.google.com/citations?user=I2eXSE0AAAAJ" target="_blank">Siting Zhu</a>, 
      <a href="https://guangmingw.github.io/" target="_blank">Guangming Wang</a>, 
      <a href="https://hermannblum.net/" target="_blank">Hermann Blum</a>, 
      <a href="https://scholar.google.com/citations?user=rrkp_usAAAAJ" target="_blank">Zhong Wang</a>, 
      <strong>Ganlin Zhang</strong>, 
      <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>,
      <a href="https://cvg.ethz.ch/team/Prof-Dr-Marc-Pollefeys" target="_blank">Marc Pollefeys</a>,
      <a href="https://irmv.sjtu.edu.cn/wanghesheng" target="_blank">Hesheng Wang</a>
      <br>
      <em><strong>TPAMI</strong> 2025</em>
      <br>
      <!-- <a href="https://github.com/" target="_blank">Github Repo</a> |  -->
      <a href="https://ieeexplore.ieee.org/document/11260914" target="_blank">Paper</a> | 
      <a href="https://irmvlab.github.io/sni-slam-plus.github.io/" target="_blank">Project website</a>
      <br>
      SNI-SLAM++ is a tightly coupled semantic SLAM system that achieves robust tracking and dense semantic mapping through hierarchical semantic encoding, cross-attention feature fusion, and a semantics-coupled tracking framework.
    </td>
  </tr>
</table>
<hr>

<heading><strong>Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="batrack">
      <div class="one">
        <div class="two shape" style="width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="https://wrchen530.github.io/projects/batrack/static/videos/davis_3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="https://wrchen530.github.io/images/batrack.png" style="width: 100%;" class="img"/>
      </div>       
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://wrchen530.github.io/projects/batrack" target="_blank">
            Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction
          </a>
        </strong>
      </papertitle>
      <br>
      <a href="https://wrchen530.github.io/" target="_blank">Weirong Chen</a>, 
      <strong>Ganlin Zhang</strong>, 
      <a href="https://fwmb.github.io/" target="_blank">Felix Wimbauer</a>, 
      <a href="https://rui2016.github.io/" target="_blank">Rui Wang</a>, 
      <a href="https://arnike.github.io/" target="_blank">Nikita Araslanov</a>, 
      <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank">Andrea Vedaldi</a>, 
      <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
      <br>
      <em><strong>ICCV</strong> 2025 <span style="color:rgb(255, 94, 0);font-weight: bold;">(Best Paper Candidate)</span></em>
      <br>
      <a href="https://github.com/wrchen530/batrack" target="_blank">Github Repo</a> | 
      <a href="https://arxiv.org/abs/2504.14516" target="_blank">ArXiv</a> | 
      <a href="https://wrchen530.github.io/projects/batrack/" target="_blank">Project website</a>
      <br>
      A method for consistent dynamic scene reconstruction via motion decoupling, bundle adjustment, and global refinement.
    </td>
  </tr>
</table>
<hr>


<heading><strong>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians</strong> </heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/splatslam.jpg" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/papers/Sandstrom_Splat-SLAM_Globally_Optimized_RGB-only_SLAM_with_3D_Gaussians_CVPRW_2025_paper.pdf" target="_blank">Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://eriksandstroem.github.io/" target="_blank">Erik Sandström*</a>, 
    <strong>Ganlin Zhang*</strong>, 
    <a href="https://scholar.google.com/citations?user=ml3laqEAAAAJ" target="_blank"> Keisuke Tateno</a>, 
    <a href="https://moechsle.github.io/" target="_blank"> Michael Oechsle</a>, 
    <a href="https://youmi-zym.github.io/" target="_blank"> Youmin Zhang</a>, 
    <a href="https://manthan99.github.io/" target="_blank"> Manthan Patel</a>, 
    <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjg3LC0xOTcxNDY1MTc4.html" target="_blank"> Luc Van Gool</a>, 
    <a href="https://oswaldm.github.io/" target="_blank"> Martin R. Oswald</a>, 
    <a href="https://federicotombari.github.io/" target="_blank"> Federico Tombari</a>
    <br>
    <em><strong>CVPRW</strong> 2025</em>
    <br>
    <a href="https://github.com/google-research/Splat-SLAM" target="_blank">Github Repo</a> | 
    <a href="https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/html/Sandstrom_Splat-SLAM_Globally_Optimized_RGB-only_SLAM_with_3D_Gaussians_CVPRW_2025_paper.html" target="_blank">Paper</a>
    <br>
    We use a keyframe based frame to frame tracker based on dense optical flow connected to a pose graph for global consistency. For dense mapping, we resort to a 3DGS representation, suitable for extracting both dense geometry and rendering from.
  </td>
</table>
<hr>

  
<heading><strong>GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM</strong></heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
    <td width="40%" data-prefix="glorie">
      <div class="one">
        <div class="two shape" style="display: flex; justify-content: center; width: 100%;">
          <video autoplay muted loop playsinline width="100%">
            <source src="/images/publications/glorie.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <img src="/images/publications/glorie.jpg" width="100%" class="img"/>
      </div>        
    </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://zhangganlin.github.io/GlORIE-SLAM/index.html" target="_blank">
            GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM
          </a>
        </strong>
      </papertitle>
      <br>
      <strong>Ganlin Zhang*</strong>, 
      <a href="https://eriksandstroem.github.io/" target="_blank">Erik Sandström*</a>, 
      <a href="https://youmi-zym.github.io/" target="_blank">Youmin Zhang</a>, 
      <a href="https://manthan99.github.io/" target="_blank">Manthan Patel</a>, 
      <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjg3LC0xOTcxNDY1MTc4.html" target="_blank">Luc Van Gool</a>, 
      <a href="https://oswaldm.github.io/" target="_blank">Martin R. Oswald</a>
      <br>
      <em>Preprint on ArXiv, 2024</em>
      <br>
      <a href="https://github.com/zhangganlin/GlORIE-SLAM" target="_blank">Github Repo</a> | 
      <a href="https://arxiv.org/abs/2403.19549" target="_blank">ArXiv</a> | 
      <a href="https://zhangganlin.github.io/GlORIE-SLAM/index.html" target="_blank">Project website</a>
      <br>
      1. A monocular SLAM pipeline with deformable neural point cloud scene representation. <br>
      2. Novel DSPO layer for BA, which can jointly optimize depth map, depth scale, and camera pose. <br>
    </td>
  </tr>
</table>
<hr>


<heading><strong>Revisiting Rotation Averaging: Uncertainties and Robust Losses</strong> </heading>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/rotationAverage.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Revisiting_Rotation_Averaging_Uncertainties_and_Robust_Losses_CVPR_2023_paper.pdf" target="_blank">Revisiting Rotation Averaging: Uncertainties and Robust Losses</a>
    </strong>
    </papertitle>
    <br>
    <strong>Ganlin Zhang</strong>,
    <a href="https://vlarsson.github.io/" target="_blank">Viktor Larsson</a>,
    <a href="https://cvg.ethz.ch/team/Dr-Daniel-Bela-Barath" target="_blank">Daniel Barath</a>
    <br>
    <em><strong>CVPR</strong> 2023</em>
    <br>
    <a href="https://github.com/zhangganlin/GlobalSfMpy" target="_blank">Github Repo</a> | 
    <a href="https://arxiv.org/abs/2303.05195" target="_blank">ArXiv</a>
    <br>
    1. Better model the underlying noise distributions by directly propagating the uncertainty from the point correspondences into the rotation averaging. <br>
    2. Integrate a variant of the MAGSAC++ loss into the rotation averaging, instead of using the classical robust losses.
  </td>
</table>
<hr>




